{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n",
    "| Pre-Activation Policy Head | Loss Function | Depth | Main Channels | 訓練集 | Public Top1 | Public Top5 | Public 加權 | Private |\n",
    "| :------------------------: | :-----------: | :---: | :-----------: | :----: | :---------: | :---------: | :---------: | :-----: |\n",
    "|             N              | Cross-Entropy |   5   |      192      |  10M   |  0.565873    |  0.861640   |  0.22763225  |    X    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-3,\n",
    "        momentum: float = 0.01,\n",
    "        affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"running_std\", torch.ones(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_features, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_features, dtype=torch.float))\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @property\n",
    "    def rmax(self) -> torch.Tensor:\n",
    "        return (2 / 35000 * self.num_batches_tracked + 25 / 35).clamp_(1.0, 3.0)\n",
    "\n",
    "    @property\n",
    "    def dmax(self) -> torch.Tensor:\n",
    "        return (5 / 20000 * self.num_batches_tracked - 25 / 20).clamp_(0.0, 5.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask is a boolean tensor used for indexing, where True values are padded\n",
    "        i.e for 3D input, mask should be of shape (batch_size, seq_len)\n",
    "        mask is used to prevent padded values from affecting the batch statistics\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        if self.training:\n",
    "            dims = [i for i in range(x.dim() - 1)]\n",
    "            if mask is not None:\n",
    "                z = x[~mask]\n",
    "                batch_mean = z.mean(0)\n",
    "                batch_std = z.std(0, unbiased=False) + self.eps\n",
    "            else:\n",
    "                batch_mean = x.mean(dims)\n",
    "                batch_std = x.std(dims, unbiased=False) + self.eps\n",
    "\n",
    "            r = (batch_std.detach() / self.running_std.view_as(batch_std)).clamp_(1 / self.rmax, self.rmax)\n",
    "            d = (\n",
    "                (batch_mean.detach() - self.running_mean.view_as(batch_mean)) / self.running_std.view_as(batch_std)\n",
    "            ).clamp_(-self.dmax, self.dmax)\n",
    "            x = (x - batch_mean) / batch_std * r + d\n",
    "            self.running_mean += self.momentum * (batch_mean.detach() - self.running_mean)\n",
    "            self.running_std += self.momentum * (batch_std.detach() - self.running_std)\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            x = (x - self.running_mean) / self.running_std\n",
    "        if self.affine:\n",
    "            x = self.weight * x + self.bias\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "eps = 1e-3\n",
    "momentum = 1e-2\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=3):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = GlobalPool()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, padding=0, bias=True),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels * 2, kernel_size=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out)\n",
    "        gammas, betas = torch.split(out, self.channels, dim=1)\n",
    "        gammas = torch.reshape(gammas, (b, c, 1, 1))\n",
    "        betas = torch.reshape(betas, (b, c, 1, 1))\n",
    "        out = self.sigmoid(gammas) * x + betas\n",
    "        return out\n",
    "\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.norm = BatchRenorm2d(in_channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.se = SEBlock(out_channels) if use_se else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        if self.se is None:\n",
    "            return self.conv_3x3(out) + self.conv_1x1(out)\n",
    "        else:\n",
    "            return self.se(out)\n",
    "\n",
    "\n",
    "class InnerResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NormActConv(channels, channels, use_se=use_se)\n",
    "        self.conv2 = NormActConv(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class NestedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        c = channels\n",
    "        c2 = c // 2\n",
    "\n",
    "        self.conv_in = NormActConv(c, c2)\n",
    "\n",
    "        self.inner_block1 = InnerResidualBlock(c2, use_se=use_se)\n",
    "        self.inner_block2 = InnerResidualBlock(c2)\n",
    "\n",
    "        self.conv_out = NormActConv(c2, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv_in(out)\n",
    "        out = self.inner_block1(out)\n",
    "        out = self.inner_block2(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.norm = BatchRenorm2d(2, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.fc = nn.Linear(2 * 19 * 19, 19 * 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.norm(self.conv(x)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 192\n",
    "\n",
    "        self.conv_in = nn.Conv2d(17, channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.blocks = []\n",
    "        for _ in range(2):\n",
    "            self.blocks += [\n",
    "                NestedResidualBlock(channels),\n",
    "                NestedResidualBlock(channels, use_se=True),\n",
    "            ]\n",
    "        self.blocks.append(NestedResidualBlock(channels))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.policy_head = PolicyHead(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.blocks(out)\n",
    "        out = self.policy_head(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"../../weight/kyu/A.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B\n",
    "\n",
    "| Pre-Activation Policy Head | Loss Function | Depth | Main Channels | 訓練集 | Public Top1 | Public Top5 | Public 加權 | Private |\n",
    "| :------------------------: | :-----------: | :---: | :-----------: | :----: | :---------: | :---------: | :---------: | :-----: |\n",
    "|             N              |  Focal Loss   |   5   |      192      |  20M   |  0.568519   |  0.863404   | 0.22847015  |    X    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-3,\n",
    "        momentum: float = 0.01,\n",
    "        affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"running_std\", torch.ones(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_features, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_features, dtype=torch.float))\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @property\n",
    "    def rmax(self) -> torch.Tensor:\n",
    "        return (2 / 35000 * self.num_batches_tracked + 25 / 35).clamp_(1.0, 3.0)\n",
    "\n",
    "    @property\n",
    "    def dmax(self) -> torch.Tensor:\n",
    "        return (5 / 20000 * self.num_batches_tracked - 25 / 20).clamp_(0.0, 5.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask is a boolean tensor used for indexing, where True values are padded\n",
    "        i.e for 3D input, mask should be of shape (batch_size, seq_len)\n",
    "        mask is used to prevent padded values from affecting the batch statistics\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        if self.training:\n",
    "            dims = [i for i in range(x.dim() - 1)]\n",
    "            if mask is not None:\n",
    "                z = x[~mask]\n",
    "                batch_mean = z.mean(0)\n",
    "                batch_std = z.std(0, unbiased=False) + self.eps\n",
    "            else:\n",
    "                batch_mean = x.mean(dims)\n",
    "                batch_std = x.std(dims, unbiased=False) + self.eps\n",
    "\n",
    "            r = (batch_std.detach() / self.running_std.view_as(batch_std)).clamp_(1 / self.rmax, self.rmax)\n",
    "            d = (\n",
    "                (batch_mean.detach() - self.running_mean.view_as(batch_mean)) / self.running_std.view_as(batch_std)\n",
    "            ).clamp_(-self.dmax, self.dmax)\n",
    "            x = (x - batch_mean) / batch_std * r + d\n",
    "            self.running_mean += self.momentum * (batch_mean.detach() - self.running_mean)\n",
    "            self.running_std += self.momentum * (batch_std.detach() - self.running_std)\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            x = (x - self.running_mean) / self.running_std\n",
    "        if self.affine:\n",
    "            x = self.weight * x + self.bias\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "eps = 1e-3\n",
    "momentum = 1e-2\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=3):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = GlobalPool()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, padding=0, bias=True),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels * 2, kernel_size=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out)\n",
    "        gammas, betas = torch.split(out, self.channels, dim=1)\n",
    "        gammas = torch.reshape(gammas, (b, c, 1, 1))\n",
    "        betas = torch.reshape(betas, (b, c, 1, 1))\n",
    "        out = self.sigmoid(gammas) * x + betas\n",
    "        return out\n",
    "\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.norm = BatchRenorm2d(in_channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.se = SEBlock(out_channels) if use_se else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        if self.se is None:\n",
    "            return self.conv_3x3(out) + self.conv_1x1(out)\n",
    "        else:\n",
    "            return self.se(out)\n",
    "\n",
    "\n",
    "class InnerResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NormActConv(channels, channels, use_se=use_se)\n",
    "        self.conv2 = NormActConv(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class NestedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        c = channels\n",
    "        c2 = c // 2\n",
    "\n",
    "        self.conv_in = NormActConv(c, c2)\n",
    "\n",
    "        self.inner_block1 = InnerResidualBlock(c2, use_se=use_se)\n",
    "        self.inner_block2 = InnerResidualBlock(c2)\n",
    "\n",
    "        self.conv_out = NormActConv(c2, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv_in(out)\n",
    "        out = self.inner_block1(out)\n",
    "        out = self.inner_block2(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.norm = BatchRenorm2d(2, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.fc = nn.Linear(2 * 19 * 19, 19 * 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.norm(self.conv(x)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 192\n",
    "\n",
    "        self.conv_in = nn.Conv2d(17, channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.blocks = []\n",
    "        for _ in range(2):\n",
    "            self.blocks += [\n",
    "                NestedResidualBlock(channels),\n",
    "                NestedResidualBlock(channels, use_se=True),\n",
    "            ]\n",
    "        self.blocks.append(NestedResidualBlock(channels))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.policy_head = PolicyHead(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.blocks(out)\n",
    "        out = self.policy_head(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"../../weight/kyu/B.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C\n",
    "\n",
    "| Pre-Activation Policy Head | Loss Function | Depth | Main Channels | 訓練集 | Public Top1 | Public Top5 | Public 加權 | Private |\n",
    "| :------------------------: | :-----------: | :---: | :-----------: | :----: | :---------: | :---------: | :---------: | :-----: |\n",
    "|             N              | Cross-Entropy |   5   |      192      |  20M   |  0.574074   |  0.865168   |  0.23003530  |    1    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-3,\n",
    "        momentum: float = 0.01,\n",
    "        affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"running_std\", torch.ones(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_features, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_features, dtype=torch.float))\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @property\n",
    "    def rmax(self) -> torch.Tensor:\n",
    "        return (2 / 35000 * self.num_batches_tracked + 25 / 35).clamp_(1.0, 3.0)\n",
    "\n",
    "    @property\n",
    "    def dmax(self) -> torch.Tensor:\n",
    "        return (5 / 20000 * self.num_batches_tracked - 25 / 20).clamp_(0.0, 5.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask is a boolean tensor used for indexing, where True values are padded\n",
    "        i.e for 3D input, mask should be of shape (batch_size, seq_len)\n",
    "        mask is used to prevent padded values from affecting the batch statistics\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        if self.training:\n",
    "            dims = [i for i in range(x.dim() - 1)]\n",
    "            if mask is not None:\n",
    "                z = x[~mask]\n",
    "                batch_mean = z.mean(0)\n",
    "                batch_std = z.std(0, unbiased=False) + self.eps\n",
    "            else:\n",
    "                batch_mean = x.mean(dims)\n",
    "                batch_std = x.std(dims, unbiased=False) + self.eps\n",
    "\n",
    "            r = (batch_std.detach() / self.running_std.view_as(batch_std)).clamp_(1 / self.rmax, self.rmax)\n",
    "            d = (\n",
    "                (batch_mean.detach() - self.running_mean.view_as(batch_mean)) / self.running_std.view_as(batch_std)\n",
    "            ).clamp_(-self.dmax, self.dmax)\n",
    "            x = (x - batch_mean) / batch_std * r + d\n",
    "            self.running_mean += self.momentum * (batch_mean.detach() - self.running_mean)\n",
    "            self.running_std += self.momentum * (batch_std.detach() - self.running_std)\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            x = (x - self.running_mean) / self.running_std\n",
    "        if self.affine:\n",
    "            x = self.weight * x + self.bias\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "eps = 1e-3\n",
    "momentum = 1e-2\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=3):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = GlobalPool()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, padding=0, bias=True),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels * 2, kernel_size=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out)\n",
    "        gammas, betas = torch.split(out, self.channels, dim=1)\n",
    "        gammas = torch.reshape(gammas, (b, c, 1, 1))\n",
    "        betas = torch.reshape(betas, (b, c, 1, 1))\n",
    "        out = self.sigmoid(gammas) * x + betas\n",
    "        return out\n",
    "\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.norm = BatchRenorm2d(in_channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.se = SEBlock(out_channels) if use_se else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        if self.se is None:\n",
    "            return self.conv_3x3(out) + self.conv_1x1(out)\n",
    "        else:\n",
    "            return self.se(out)\n",
    "\n",
    "\n",
    "class InnerResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NormActConv(channels, channels, use_se=use_se)\n",
    "        self.conv2 = NormActConv(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class NestedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        c = channels\n",
    "        c2 = c // 2\n",
    "\n",
    "        self.conv_in = NormActConv(c, c2)\n",
    "\n",
    "        self.inner_block1 = InnerResidualBlock(c2, use_se=use_se)\n",
    "        self.inner_block2 = InnerResidualBlock(c2)\n",
    "\n",
    "        self.conv_out = NormActConv(c2, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv_in(out)\n",
    "        out = self.inner_block1(out)\n",
    "        out = self.inner_block2(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.norm = BatchRenorm2d(2, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.fc = nn.Linear(2 * 19 * 19, 19 * 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.norm(self.conv(x)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 192\n",
    "\n",
    "        self.conv_in = nn.Conv2d(17, channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.blocks = []\n",
    "        for _ in range(2):\n",
    "            self.blocks += [\n",
    "                NestedResidualBlock(channels),\n",
    "                NestedResidualBlock(channels, use_se=True),\n",
    "            ]\n",
    "        self.blocks.append(NestedResidualBlock(channels))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.policy_head = PolicyHead(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.blocks(out)\n",
    "        out = self.policy_head(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"../../weight/kyu/C.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D\n",
    "\n",
    "| Pre-Activation Policy Head | Loss Function | Depth | Main Channels | 訓練集 | Public Top1 | Public Top5 | Public 加權 | Private |\n",
    "| :------------------------: | :-----------: | :---: | :-----------: | :----: | :---------: | :---------: | :---------: | :-----: |\n",
    "|             N              |  Cross-Entropy   |   7   |      192      |  10M   |  0.568342   |  0.863051   | 0.22839060  |    X    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-3,\n",
    "        momentum: float = 0.01,\n",
    "        affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"running_std\", torch.ones(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_features, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_features, dtype=torch.float))\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @property\n",
    "    def rmax(self) -> torch.Tensor:\n",
    "        return (2 / 35000 * self.num_batches_tracked + 25 / 35).clamp_(1.0, 3.0)\n",
    "\n",
    "    @property\n",
    "    def dmax(self) -> torch.Tensor:\n",
    "        return (5 / 20000 * self.num_batches_tracked - 25 / 20).clamp_(0.0, 5.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask is a boolean tensor used for indexing, where True values are padded\n",
    "        i.e for 3D input, mask should be of shape (batch_size, seq_len)\n",
    "        mask is used to prevent padded values from affecting the batch statistics\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        if self.training:\n",
    "            dims = [i for i in range(x.dim() - 1)]\n",
    "            if mask is not None:\n",
    "                z = x[~mask]\n",
    "                batch_mean = z.mean(0)\n",
    "                batch_std = z.std(0, unbiased=False) + self.eps\n",
    "            else:\n",
    "                batch_mean = x.mean(dims)\n",
    "                batch_std = x.std(dims, unbiased=False) + self.eps\n",
    "\n",
    "            r = (batch_std.detach() / self.running_std.view_as(batch_std)).clamp_(1 / self.rmax, self.rmax)\n",
    "            d = (\n",
    "                (batch_mean.detach() - self.running_mean.view_as(batch_mean)) / self.running_std.view_as(batch_std)\n",
    "            ).clamp_(-self.dmax, self.dmax)\n",
    "            x = (x - batch_mean) / batch_std * r + d\n",
    "            self.running_mean += self.momentum * (batch_mean.detach() - self.running_mean)\n",
    "            self.running_std += self.momentum * (batch_std.detach() - self.running_std)\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            x = (x - self.running_mean) / self.running_std\n",
    "        if self.affine:\n",
    "            x = self.weight * x + self.bias\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "eps = 1e-3\n",
    "momentum = 1e-2\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=3):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = GlobalPool()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, padding=0, bias=True),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels * 2, kernel_size=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out)\n",
    "        gammas, betas = torch.split(out, self.channels, dim=1)\n",
    "        gammas = torch.reshape(gammas, (b, c, 1, 1))\n",
    "        betas = torch.reshape(betas, (b, c, 1, 1))\n",
    "        out = self.sigmoid(gammas) * x + betas\n",
    "        return out\n",
    "\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.norm = BatchRenorm2d(in_channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.se = SEBlock(out_channels) if use_se else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        if self.se is None:\n",
    "            return self.conv_3x3(out) + self.conv_1x1(out)\n",
    "        else:\n",
    "            return self.se(out)\n",
    "\n",
    "\n",
    "class InnerResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NormActConv(channels, channels, use_se=use_se)\n",
    "        self.conv2 = NormActConv(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class NestedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        c = channels\n",
    "        c2 = c // 2\n",
    "\n",
    "        self.conv_in = NormActConv(c, c2)\n",
    "\n",
    "        self.inner_block1 = InnerResidualBlock(c2, use_se=use_se)\n",
    "        self.inner_block2 = InnerResidualBlock(c2)\n",
    "\n",
    "        self.conv_out = NormActConv(c2, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv_in(out)\n",
    "        out = self.inner_block1(out)\n",
    "        out = self.inner_block2(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.norm = BatchRenorm2d(2, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.fc = nn.Linear(2 * 19 * 19, 19 * 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.norm(self.conv(x)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 192\n",
    "\n",
    "        self.conv_in = nn.Conv2d(17, channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.blocks = []\n",
    "        for _ in range(3):\n",
    "            self.blocks += [\n",
    "                NestedResidualBlock(channels),\n",
    "                NestedResidualBlock(channels, use_se=True),\n",
    "            ]\n",
    "        self.blocks.append(NestedResidualBlock(channels))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.policy_head = PolicyHead(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.blocks(out)\n",
    "        out = self.policy_head(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"../../weight/kyu/D.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E\n",
    "\n",
    "| Pre-Activation Policy Head | Loss Function | Depth | Main Channels | 訓練集 | Public Top1 | Public Top5 | Public 加權 | Private |\n",
    "| :------------------------: | :-----------: | :---: | :-----------: | :----: | :---------: | :---------: | :---------: | :-----: |\n",
    "|             N              |  Cross-Entropy  |   7   |      192      |  20M   |   0.571958    |  0.863757   | 0.22936520  |    2    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-3,\n",
    "        momentum: float = 0.01,\n",
    "        affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"running_std\", torch.ones(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_features, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_features, dtype=torch.float))\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @property\n",
    "    def rmax(self) -> torch.Tensor:\n",
    "        return (2 / 35000 * self.num_batches_tracked + 25 / 35).clamp_(1.0, 3.0)\n",
    "\n",
    "    @property\n",
    "    def dmax(self) -> torch.Tensor:\n",
    "        return (5 / 20000 * self.num_batches_tracked - 25 / 20).clamp_(0.0, 5.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask is a boolean tensor used for indexing, where True values are padded\n",
    "        i.e for 3D input, mask should be of shape (batch_size, seq_len)\n",
    "        mask is used to prevent padded values from affecting the batch statistics\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        if self.training:\n",
    "            dims = [i for i in range(x.dim() - 1)]\n",
    "            if mask is not None:\n",
    "                z = x[~mask]\n",
    "                batch_mean = z.mean(0)\n",
    "                batch_std = z.std(0, unbiased=False) + self.eps\n",
    "            else:\n",
    "                batch_mean = x.mean(dims)\n",
    "                batch_std = x.std(dims, unbiased=False) + self.eps\n",
    "\n",
    "            r = (batch_std.detach() / self.running_std.view_as(batch_std)).clamp_(1 / self.rmax, self.rmax)\n",
    "            d = (\n",
    "                (batch_mean.detach() - self.running_mean.view_as(batch_mean)) / self.running_std.view_as(batch_std)\n",
    "            ).clamp_(-self.dmax, self.dmax)\n",
    "            x = (x - batch_mean) / batch_std * r + d\n",
    "            self.running_mean += self.momentum * (batch_mean.detach() - self.running_mean)\n",
    "            self.running_std += self.momentum * (batch_std.detach() - self.running_std)\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            x = (x - self.running_mean) / self.running_std\n",
    "        if self.affine:\n",
    "            x = self.weight * x + self.bias\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "eps = 1e-3\n",
    "momentum = 1e-2\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=3):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = GlobalPool()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, padding=0, bias=True),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels * 2, kernel_size=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out)\n",
    "        gammas, betas = torch.split(out, self.channels, dim=1)\n",
    "        gammas = torch.reshape(gammas, (b, c, 1, 1))\n",
    "        betas = torch.reshape(betas, (b, c, 1, 1))\n",
    "        out = self.sigmoid(gammas) * x + betas\n",
    "        return out\n",
    "\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.norm = BatchRenorm2d(in_channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.se = SEBlock(out_channels) if use_se else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        if self.se is None:\n",
    "            return self.conv_3x3(out) + self.conv_1x1(out)\n",
    "        else:\n",
    "            return self.se(out)\n",
    "\n",
    "\n",
    "class InnerResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NormActConv(channels, channels, use_se=use_se)\n",
    "        self.conv2 = NormActConv(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class NestedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        c = channels\n",
    "        c2 = c // 2\n",
    "\n",
    "        self.conv_in = NormActConv(c, c2)\n",
    "\n",
    "        self.inner_block1 = InnerResidualBlock(c2, use_se=use_se)\n",
    "        self.inner_block2 = InnerResidualBlock(c2)\n",
    "\n",
    "        self.conv_out = NormActConv(c2, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv_in(out)\n",
    "        out = self.inner_block1(out)\n",
    "        out = self.inner_block2(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.norm = BatchRenorm2d(2, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.fc = nn.Linear(2 * 19 * 19, 19 * 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.norm(self.conv(x)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 192\n",
    "\n",
    "        self.conv_in = nn.Conv2d(17, channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.blocks = []\n",
    "        for _ in range(3):\n",
    "            self.blocks += [\n",
    "                NestedResidualBlock(channels),\n",
    "                NestedResidualBlock(channels, use_se=True),\n",
    "            ]\n",
    "        self.blocks.append(NestedResidualBlock(channels))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.policy_head = PolicyHead(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.blocks(out)\n",
    "        out = self.policy_head(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"../../weight/kyu/E.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F\n",
    "\n",
    "| Pre-Activation Policy Head | Loss Function | Depth | Main Channels | 訓練集 | Public Top1 | Public Top5 | Public 加權 | Private |\n",
    "| :------------------------: | :-----------: | :---: | :-----------: | :----: | :---------: | :---------: | :---------: | :-----: |\n",
    "|             Y              |  Cross-Entropy  |   5   |      192      |  10M   |  0.571693   |  0.861552  | 0.22907845  |    4    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-3,\n",
    "        momentum: float = 0.01,\n",
    "        affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"running_std\", torch.ones(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_features, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_features, dtype=torch.float))\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @property\n",
    "    def rmax(self) -> torch.Tensor:\n",
    "        return (2 / 35000 * self.num_batches_tracked + 25 / 35).clamp_(1.0, 3.0)\n",
    "\n",
    "    @property\n",
    "    def dmax(self) -> torch.Tensor:\n",
    "        return (5 / 20000 * self.num_batches_tracked - 25 / 20).clamp_(0.0, 5.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask is a boolean tensor used for indexing, where True values are padded\n",
    "        i.e for 3D input, mask should be of shape (batch_size, seq_len)\n",
    "        mask is used to prevent padded values from affecting the batch statistics\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        if self.training:\n",
    "            dims = [i for i in range(x.dim() - 1)]\n",
    "            if mask is not None:\n",
    "                z = x[~mask]\n",
    "                batch_mean = z.mean(0)\n",
    "                batch_std = z.std(0, unbiased=False) + self.eps\n",
    "            else:\n",
    "                batch_mean = x.mean(dims)\n",
    "                batch_std = x.std(dims, unbiased=False) + self.eps\n",
    "\n",
    "            r = (batch_std.detach() / self.running_std.view_as(batch_std)).clamp_(1 / self.rmax, self.rmax)\n",
    "            d = (\n",
    "                (batch_mean.detach() - self.running_mean.view_as(batch_mean)) / self.running_std.view_as(batch_std)\n",
    "            ).clamp_(-self.dmax, self.dmax)\n",
    "            x = (x - batch_mean) / batch_std * r + d\n",
    "            self.running_mean += self.momentum * (batch_mean.detach() - self.running_mean)\n",
    "            self.running_std += self.momentum * (batch_std.detach() - self.running_std)\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            x = (x - self.running_mean) / self.running_std\n",
    "        if self.affine:\n",
    "            x = self.weight * x + self.bias\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "eps = 1e-3\n",
    "momentum = 1e-2\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=3):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = GlobalPool()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, padding=0, bias=True),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels * 2, kernel_size=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out)\n",
    "        gammas, betas = torch.split(out, self.channels, dim=1)\n",
    "        gammas = torch.reshape(gammas, (b, c, 1, 1))\n",
    "        betas = torch.reshape(betas, (b, c, 1, 1))\n",
    "        out = self.sigmoid(gammas) * x + betas\n",
    "        return out\n",
    "\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.norm = BatchRenorm2d(in_channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.se = SEBlock(out_channels) if use_se else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        if self.se is None:\n",
    "            return self.conv_3x3(out) + self.conv_1x1(out)\n",
    "        else:\n",
    "            return self.se(out)\n",
    "\n",
    "\n",
    "class InnerResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NormActConv(channels, channels, use_se=use_se)\n",
    "        self.conv2 = NormActConv(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class NestedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        c = channels\n",
    "        c2 = c // 2\n",
    "\n",
    "        self.conv_in = NormActConv(c, c2)\n",
    "\n",
    "        self.inner_block1 = InnerResidualBlock(c2, use_se=use_se)\n",
    "        self.inner_block2 = InnerResidualBlock(c2)\n",
    "\n",
    "        self.conv_out = NormActConv(c2, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv_in(out)\n",
    "        out = self.inner_block1(out)\n",
    "        out = self.inner_block2(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.norm = BatchRenorm2d(channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.fc = nn.Linear(2 * 19 * 19, 19 * 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(self.act(self.norm(x)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 192\n",
    "\n",
    "        self.conv_in = nn.Conv2d(17, channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.blocks = []\n",
    "        for _ in range(2):\n",
    "            self.blocks += [\n",
    "                NestedResidualBlock(channels),\n",
    "                NestedResidualBlock(channels, use_se=True),\n",
    "            ]\n",
    "        self.blocks.append(NestedResidualBlock(channels))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.policy_head = PolicyHead(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.blocks(out)\n",
    "        out = self.policy_head(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"../../weight/kyu/F.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G\n",
    "\n",
    "| Pre-Activation Policy Head | Loss Function | Depth | Main Channels | 訓練集 | Public Top1 | Public Top5 | Public 加權 | Private |\n",
    "| :------------------------: | :-----------: | :---: | :-----------: | :----: | :---------: | :---------: | :---------: | :-----: |\n",
    "|             Y              |  Cross-Entropy   |   5   |      192      |  20M   |  0.575661   |  0.862257   | 0.23014095  |    3    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BatchRenorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-3,\n",
    "        momentum: float = 0.01,\n",
    "        affine: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"running_std\", torch.ones(num_features, dtype=torch.float))\n",
    "        self.register_buffer(\"num_batches_tracked\", torch.tensor(0, dtype=torch.long))\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_features, dtype=torch.float))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_features, dtype=torch.float))\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    @property\n",
    "    def rmax(self) -> torch.Tensor:\n",
    "        return (2 / 35000 * self.num_batches_tracked + 25 / 35).clamp_(1.0, 3.0)\n",
    "\n",
    "    @property\n",
    "    def dmax(self) -> torch.Tensor:\n",
    "        return (5 / 20000 * self.num_batches_tracked - 25 / 20).clamp_(0.0, 5.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mask is a boolean tensor used for indexing, where True values are padded\n",
    "        i.e for 3D input, mask should be of shape (batch_size, seq_len)\n",
    "        mask is used to prevent padded values from affecting the batch statistics\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        if self.training:\n",
    "            dims = [i for i in range(x.dim() - 1)]\n",
    "            if mask is not None:\n",
    "                z = x[~mask]\n",
    "                batch_mean = z.mean(0)\n",
    "                batch_std = z.std(0, unbiased=False) + self.eps\n",
    "            else:\n",
    "                batch_mean = x.mean(dims)\n",
    "                batch_std = x.std(dims, unbiased=False) + self.eps\n",
    "\n",
    "            r = (batch_std.detach() / self.running_std.view_as(batch_std)).clamp_(1 / self.rmax, self.rmax)\n",
    "            d = (\n",
    "                (batch_mean.detach() - self.running_mean.view_as(batch_mean)) / self.running_std.view_as(batch_std)\n",
    "            ).clamp_(-self.dmax, self.dmax)\n",
    "            x = (x - batch_mean) / batch_std * r + d\n",
    "            self.running_mean += self.momentum * (batch_mean.detach() - self.running_mean)\n",
    "            self.running_std += self.momentum * (batch_std.detach() - self.running_std)\n",
    "            self.num_batches_tracked += 1\n",
    "        else:\n",
    "            x = (x - self.running_mean) / self.running_std\n",
    "        if self.affine:\n",
    "            x = self.weight * x + self.bias\n",
    "        if x.dim() > 2:\n",
    "            x = x.transpose(1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "eps = 1e-3\n",
    "momentum = 1e-2\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)\n",
    "        max_out = self.max_pool(x)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=3):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.pool = GlobalPool()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, padding=0, bias=True),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels * 2, kernel_size=1, padding=0, bias=True),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        out = self.pool(x)\n",
    "        out = self.conv(out)\n",
    "        gammas, betas = torch.split(out, self.channels, dim=1)\n",
    "        gammas = torch.reshape(gammas, (b, c, 1, 1))\n",
    "        betas = torch.reshape(betas, (b, c, 1, 1))\n",
    "        out = self.sigmoid(gammas) * x + betas\n",
    "        return out\n",
    "\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_se=False):\n",
    "        super().__init__()\n",
    "        self.norm = BatchRenorm2d(in_channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False)\n",
    "        self.se = SEBlock(out_channels) if use_se else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        if self.se is None:\n",
    "            return self.conv_3x3(out) + self.conv_1x1(out)\n",
    "        else:\n",
    "            return self.se(out)\n",
    "\n",
    "\n",
    "class InnerResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = NormActConv(channels, channels, use_se=use_se)\n",
    "        self.conv2 = NormActConv(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class NestedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False):\n",
    "        super().__init__()\n",
    "\n",
    "        c = channels\n",
    "        c2 = c // 2\n",
    "\n",
    "        self.conv_in = NormActConv(c, c2)\n",
    "\n",
    "        self.inner_block1 = InnerResidualBlock(c2, use_se=use_se)\n",
    "        self.inner_block2 = InnerResidualBlock(c2)\n",
    "\n",
    "        self.conv_out = NormActConv(c2, c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.conv_in(out)\n",
    "        out = self.inner_block1(out)\n",
    "        out = self.inner_block2(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.norm = BatchRenorm2d(channels, eps=eps, momentum=momentum)\n",
    "        self.act = nn.Mish(inplace=True)\n",
    "        self.fc = nn.Linear(2 * 19 * 19, 19 * 19)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(self.act(self.norm(x)))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = 192\n",
    "\n",
    "        self.conv_in = nn.Conv2d(17, channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.blocks = []\n",
    "        for _ in range(2):\n",
    "            self.blocks += [\n",
    "                NestedResidualBlock(channels),\n",
    "                NestedResidualBlock(channels, use_se=True),\n",
    "            ]\n",
    "        self.blocks.append(NestedResidualBlock(channels))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.policy_head = PolicyHead(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_in(x)\n",
    "        out = self.blocks(out)\n",
    "        out = self.policy_head(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"../../weight//kyu/G.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
